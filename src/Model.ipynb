{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c10e641-fd18-4812-9930-dbb5d251cabf",
   "metadata": {},
   "source": [
    "##**Place where we tune the max_features**##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e14a3d-6dd5-4c53-8637-39cd78ca8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"boss_email_dataset_encode_change_after_filter.csv\")\n",
    "\n",
    "# Split the data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['subject', 'body', 'sender']], df['is_phishing'], test_size=0.2, random_state=100)\n",
    "\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
    "    'Random Forest': RandomForestClassifier(random_state=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=100),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Parameter grid for max_features\n",
    "max_features_grid = [1000, 1500, 2000]\n",
    "\n",
    "# Function to run grid search on both \"subject\" and \"body\" for all models\n",
    "def run_grid_search(models, X_train, y_train):\n",
    "    results = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Running Grid Search for model: {model_name}\")\n",
    "        \n",
    "        # Create a pipeline for the current model\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "        \n",
    "        # Define the parameter grid for max_features\n",
    "        param_grid = {\n",
    "            'tfidf__max_features': max_features_grid\n",
    "        }\n",
    "        \n",
    "        # Grid search for the \"subject\" column\n",
    "        grid_search_subject = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search_subject.fit(X_train['subject'], y_train)\n",
    "        \n",
    "        # Store results for \"subject\"\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'text_column': 'subject',\n",
    "            'best_feature': grid_search_subject.best_params_,\n",
    "            'best_score': grid_search_subject.best_score_\n",
    "        })\n",
    "        \n",
    "        # Grid search for the \"body\" column\n",
    "        grid_search_body = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search_body.fit(X_train['body'], y_train)\n",
    "        \n",
    "        # Store results for \"body\"\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'text_column': 'body',\n",
    "            'best_feature': grid_search_body.best_params_,\n",
    "            'best_score': grid_search_body.best_score_\n",
    "        })\n",
    "    \n",
    "    # Print all results\n",
    "    for result in results:\n",
    "        print(f\"\\nModel: {result['model']} | Text Column: {result['text_column']}\")\n",
    "        print(f\"Best max_features: {result['best_feature']['tfidf__max_features']}\")\n",
    "        print(f\"Best accuracy score: {result['best_score']:.4f}\")\n",
    "\n",
    "# Run grid search across all models and both \"subject\" and \"body\" columns\n",
    "run_grid_search(models, X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afdae1-f4e1-407c-ba1e-f4b92d582e5c",
   "metadata": {},
   "source": [
    "#Based on the 2000 max features that we selected to vectorize X-values, feed them into the ML models with default settings#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103e4ab-b34e-4b88-9894-a9f31c824b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"boss_email_dataset_encode_change_after_filter.csv\")\n",
    "\n",
    "# Split the data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['subject', 'body', 'sender']], df['is_phishing'], test_size=0.2, random_state=100)\n",
    "\n",
    "# Step 1: Vectorize the 'subject' column using TF-IDF, we do it seperately for test and train, to prevent data leakage, and prevent model from learning\n",
    "tfidf_subject = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "X_train_subject = tfidf_subject.fit_transform(X_train['subject']).toarray()\n",
    "X_test_subject = tfidf_subject.transform(X_test['subject']).toarray()\n",
    "\n",
    "# Step 2: Vectorize the 'body' column using TF-IDF\n",
    "tfidf_body = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "X_train_body = tfidf_body.fit_transform(X_train['body']).toarray()\n",
    "X_test_body = tfidf_body.transform(X_test['body']).toarray()\n",
    "\n",
    "# Step 3: Encode the 'sender' column using One-Hot Encoding, similarly, we do it seperately for test and train. If theres a new sender in testing set, the handle_unkown will ignore it\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_sender = encoder.fit_transform(X_train[['sender']]).toarray()\n",
    "X_test_sender = encoder.transform(X_test[['sender']]).toarray()\n",
    "\n",
    "# Combine transformed features\n",
    "X_train_combined = np.hstack([X_train_subject, X_train_body, X_train_sender])\n",
    "X_test_combined = np.hstack([X_test_subject, X_test_body, X_test_sender])\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nConfusion Matrix for {model_name}:\\n\", cm)\n",
    "\n",
    "# ------------------------ Logistic regression ------------------------\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train_combined, y_train) #Where The model learns from the training set \n",
    "y_pred_log_reg = log_reg.predict(X_test_combined) #model predicts Y_test using the x_test set\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_log_reg:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_log_reg, \"Logistic Regression\")\n",
    "\n",
    "# ------------------------ Gradient Boosting ------------------------\n",
    "gb_model = GradientBoostingClassifier(random_state=100)\n",
    "gb_model.fit(X_train_combined, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_combined)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"\\nGradient Boosting Accuracy: {accuracy_gb:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_gb, \"Gradient Boosting\")\n",
    "\n",
    "# ------------------------ Random Forest ------------------------\n",
    "rf_model = RandomForestClassifier(random_state=100)\n",
    "rf_model.fit(X_train_combined, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_combined)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nRandom Forest Accuracy: {accuracy_rf:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# ------------------------ Naive Bayes ------------------------\n",
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train_combined, y_train)\n",
    "y_pred_nb = naive_bayes_model.predict(X_test_combined)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"\\nNaive Bayes Accuracy: {accuracy_nb:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_nb, \"Naive Bayes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd64aac-1f7f-4a91-9dec-ff824e0ce2bb",
   "metadata": {},
   "source": [
    "##Further Hyperparameter tuning##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240d7fdd-8d7a-4b75-9998-c313357a30bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF Vectorization of 'subject' column...\n",
      "Starting TF-IDF Vectorization of 'body' column...\n",
      "Starting One-Hot Encoding of 'sender' column...\n",
      "Combining all transformed features...\n",
      "Starting Grid Search for Logistic Regression...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best Hyperparameters for Logistic Regression: {'C': 10, 'penalty': 'l2'}\n",
      "Logistic Regression Accuracy (Tuned): 99.12%\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      " [[1364    5]\n",
      " [  11  432]]\n",
      "Starting Grid Search for Gradient Boosting...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      "Best Hyperparameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100}\n",
      "\n",
      "Gradient Boosting Accuracy (Tuned): 98.40%\n",
      "\n",
      "Confusion Matrix for Gradient Boosting:\n",
      " [[1365    4]\n",
      " [  25  418]]\n",
      "Starting Grid Search for Random Forest...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best Hyperparameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "Random Forest Accuracy (Tuned): 98.40%\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      " [[1366    3]\n",
      " [  26  417]]\n",
      "Starting Grid Search for Naive Bayes...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Best Hyperparameters for Naives Bayes: {'alpha': 0.1, 'fit_prior': False}\n",
      "\n",
      "Naive Bayes Accuracy (Tuned): 97.57%\n",
      "\n",
      "Confusion Matrix for Naive Bayes:\n",
      " [[1338   31]\n",
      " [  13  430]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"boss_email_dataset_encode_change_after_filter.csv\")\n",
    "\n",
    "# Split the data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['subject', 'body', 'sender']], df['is_phishing'], test_size=0.2, random_state=100)\n",
    "\n",
    "# Step 1: Vectorize the 'subject' column using TF-IDF\n",
    "print(\"Starting TF-IDF Vectorization of 'subject' column...\")\n",
    "tfidf_subject = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_subject = tfidf_subject.fit_transform(X_train['subject']).toarray()\n",
    "X_test_subject = tfidf_subject.transform(X_test['subject']).toarray()\n",
    "\n",
    "# Step 2: Vectorize the 'body' column using TF-IDF\n",
    "print(\"Starting TF-IDF Vectorization of 'body' column...\")\n",
    "tfidf_body = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_body = tfidf_body.fit_transform(X_train['body']).toarray()\n",
    "X_test_body = tfidf_body.transform(X_test['body']).toarray()\n",
    "\n",
    "# Step 3: Encode the 'sender' column using One-Hot Encoding\n",
    "print(\"Starting One-Hot Encoding of 'sender' column...\")\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_sender = encoder.fit_transform(X_train[['sender']]).toarray()\n",
    "X_test_sender = encoder.transform(X_test[['sender']]).toarray()\n",
    "\n",
    "# Combine transformed features\n",
    "print(\"Combining all transformed features...\")\n",
    "X_train_combined = np.hstack([X_train_subject, X_train_body, X_train_sender])\n",
    "X_test_combined = np.hstack([X_test_subject, X_test_body, X_test_sender])\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nConfusion Matrix for {model_name}:\\n\", cm)\n",
    "\n",
    "# ------------------------ Hyperparameter Tuning ------------------------\n",
    "\n",
    "# Parameter grids for each model\n",
    "param_grid_log_reg = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'penalty': ['l2'],        # Use 'l1' or 'l2' if using solver='liblinear'\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [None, 10, 20],     # Maximum depth of tree\n",
    "    'min_samples_split': [2, 5],     # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2]       # Minimum samples at a leaf node\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100],  # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate for each stage\n",
    "    'max_depth': [3, 5, 7]  # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1, 0.5, 1.0],  # Smoothing parameter\n",
    "    'fit_prior': [True, False]  # Learn class prior probabilities or not\n",
    "}\n",
    "\n",
    "# ------------------------ Logistic Regression ------------------------\n",
    "print(\"Starting Grid Search for Logistic Regression...\")\n",
    "log_reg = LogisticRegression(max_iter=200, solver='liblinear')\n",
    "grid_search_log_reg = GridSearchCV(log_reg, param_grid_log_reg, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search_log_reg.fit(X_train_combined, y_train)\n",
    "best_log_reg = grid_search_log_reg.best_estimator_\n",
    "\n",
    "y_pred_log_reg = best_log_reg.predict(X_test_combined)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"\\nBest Hyperparameters for Logistic Regression: {grid_search_log_reg.best_params_}\")\n",
    "print(f\"Logistic Regression Accuracy (Tuned): {accuracy_log_reg:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_log_reg, \"Logistic Regression\")\n",
    "\n",
    "# ------------------------ Gradient Boosting ------------------------\n",
    "print(\"Starting Grid Search for Gradient Boosting...\")\n",
    "gb_model = GradientBoostingClassifier(random_state=100)\n",
    "grid_search_gb = GridSearchCV(gb_model, param_grid_gb, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search_gb.fit(X_train_combined, y_train)\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "\n",
    "y_pred_gb = best_gb_model.predict(X_test_combined)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"\\nBest Hyperparameters for Gradient Boosting: {grid_search_gb.best_params_}\")\n",
    "print(f\"\\nGradient Boosting Accuracy (Tuned): {accuracy_gb:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_gb, \"Gradient Boosting\")\n",
    "\n",
    "# ------------------------ Random Forest ------------------------\n",
    "print(\"Starting Grid Search for Random Forest...\")\n",
    "rf_model = RandomForestClassifier(random_state=100)\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train_combined, y_train)\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "y_pred_rf = best_rf_model.predict(X_test_combined)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nBest Hyperparameters for Random Forest: {grid_search_rf.best_params_}\")\n",
    "print(f\"\\nRandom Forest Accuracy (Tuned): {accuracy_rf:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# ------------------------ Naive Bayes ------------------------\n",
    "print(\"Starting Grid Search for Naive Bayes...\")\n",
    "naive_bayes_model = MultinomialNB()\n",
    "grid_search_nb = GridSearchCV(naive_bayes_model, param_grid_nb, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search_nb.fit(X_train_combined, y_train)\n",
    "best_nb_model = grid_search_nb.best_estimator_\n",
    "\n",
    "y_pred_nb = best_nb_model.predict(X_test_combined)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"\\nBest Hyperparameters for Naives Bayes: {grid_search_nb.best_params_}\")\n",
    "print(f\"\\nNaive Bayes Accuracy (Tuned): {accuracy_nb:.2%}\")\n",
    "print_confusion_matrix(y_test, y_pred_nb, \"Naive Bayes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51037d9b-ca66-4d45-8f23-682dc628978c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
